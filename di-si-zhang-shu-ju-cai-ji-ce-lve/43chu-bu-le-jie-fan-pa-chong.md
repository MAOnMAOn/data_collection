## 4.3 初步了解反爬虫

网络爬虫的实质，其实是从网络上“偷”数据。通过网络爬虫，我们可以采集到所需要的资源，但是同样，使用不当也可能会引发一些比较严重的问题。当前随着爬虫技术的不断发展，发爬虫行为也越来越多，现在我们就简单地关注一下反爬虫话题。

### 1. 为什么要反爬虫
目标网站对爬虫采取反制措施，无非是基于以下几点考虑：
 1. 爬虫影响服务器性能，对服务器产生骚扰行为，并加大了网站维护者的工作量，除了机器，人工也费钱。
 2. 网络爬虫也可能会造成用户的隐私泄露。
 3. 如果没有反爬虫，竞争对手就可以批量复制相关数据，降低自身市场竞争力。
 4. 爬虫行为在法律上还没有得到有效的界定（擦边球）。

### 2. 反什么样的爬虫
***1、十分低级的入门级的爬虫***
目前网络上的爬虫资料通常都只是给一段 python 代码，搜到的 python 爬虫代码除了带一个简单的user-agent，并没有太多的优化。这些爬虫在带来大部分非正常流量的同时，也是目标网站最容易反制的爬虫。

***2、失控的小爬虫***
有些爬虫，很可能就是一些托管在某些服务器上的小爬虫，已经无人认领了，但依然在辛勤地工作着，这些爬虫也是目标站点需要反爬的。

***3、成型的商业对手***
商业对手有技术，有钱，要什么有什么，一旦死磕的话......

### 3. 爬虫和反爬虫的相关定义
爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。
反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。
误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。
拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。
资源：机器成本与人力成本的总和，人力成本也是资源，而且比机器更重要。

### 4. 网站识别爬虫的手段

***传统反爬虫手段***

*1、后台对访问进行统计，如果单个IP访问超过阈值，予以封锁。*

>虽然效果不错，但是其实有两个缺陷，一个是非常容易误伤普通用户，另一个就是，IP其实不值钱，几十块钱甚至有可能买到几十万个IP。

*2、后台对访问进行统计，如果单个session访问超过阈值，予以封锁。*

>实际效果更差，因为session完全不值钱，重新申请一个就可以了。

*3、后台对访问进行统计，如果单个userAgent访问超过阈值，予以封锁。*

>效果很好，但是杀伤力过大，误伤非常严重。

*4、强制用户登录*

*5、以上的组合*

组合起来能力变大，误伤率下降，针对低级爬虫还是比较有效。

***新型反爬手段***
传统的手段效果均一般，网站反爬虫开始越来越多的使用以下手段：

 1. JavaScript 技术；
 2. 浏览器检测，针对不同的浏览器，使用不一样的检测方式；
 3. 验证码，比如当前的极验验证码；
 4. 通过机器学习，比如淘宝天猫的反爬虫